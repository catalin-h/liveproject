{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QiORDZJhJvl"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification (Manning Publications)</span><br/>by <em>Daniel Buscombe</em></strong><br/><br/>\n",
    "        <strong>> Chapter 4: Model Training and Evaluation </strong><br/>\n",
    "    </p>           \n",
    "        \n",
    "<p style=\"border: 1px solid #182AEB; border-left: 15px solid #182AEB; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #182AEB\">What you learned in Part 3.</strong>  \n",
    "    <br/>In Part 3, you learned how to set up and merge image generators, carry out data augmentation, put together and train deep learning models for denoising imagery using keras/Tensorflow 2, and put together a trainable UNet model implementation for semantic segmentation.\n",
    "    </p>\n",
    "    \n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">What you will learn in this Part.</strong>  \n",
    "    <br/>In Part 4, you will first learn a few core concepts in training deep learning models, create checkpoints and callback functions for training your UNet model, and then train that model on a non-augmented and augmented Sentinel2-cloudless dataset. Finally, you will plot the training histories of the model training and validation, and use the model on a test image for prediction.\n",
    "    </p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGuVCorYWdTj"
   },
   "source": [
    "Switch to (or install) tensorflow 2 (if you are on colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WWkBEW6hJvv"
   },
   "outputs": [],
   "source": [
    "colab = 0\n",
    "#colab = 1\n",
    "\n",
    "if colab==1:\n",
    "    %tensorflow_version 2.x\n",
    "    #!pip install --default-timeout=1000 tensorflow-gpu==2.0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdbsMkTKjF9T"
   },
   "source": [
    "You may have to restart the runtime and/or change runtime type here, if the following doesn't show Tensorflow version 2, and a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQFWD5f3mHcG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3BiX8TxmLVz"
   },
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NvkSGwdhJvy"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Key concepts in  <br/> training deep learning models</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "A a high level, model training consists of, upon each successive iteration:\n",
    "\n",
    "* Evaluate the current model but computing its _loss_, which is the mismatch between the a batch of validation labels and the model's current estimate of those labels\n",
    "\n",
    "* Adjust the weights of the network (the parameters of the model) to try to reduce the loss\n",
    "\n",
    "* Repeat the above until the model solution converges (it cannot be improved)\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eI7lZC53hJv5"
   },
   "source": [
    "### Backpropagation\n",
    "Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. \n",
    "\n",
    "### Updating weights\n",
    "In a neural network, weights are updated as follows:\n",
    "\n",
    "* Step 1: Take a batch of training data and perform forward propagation to compute the loss. \n",
    "* Step 2: Backpropagate the loss to get the gradient of the loss with respect to each weight. \n",
    "* Step 3: Use the gradients to update the weights of the network.\n",
    "\n",
    "![](https://stanford.edu/~shervine/images/update-weights.png)\n",
    "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)\n",
    "\n",
    "### Optimizing convergence\n",
    "\n",
    "Stochastic Gradient Descent is a popular way to find how to minimize a cost function  (called finding a global minima)\n",
    "\n",
    "For each example in the data:\n",
    "\n",
    "* find the value predicted by the neural network \n",
    "* calculate the loss from the loss function \n",
    "* find partial derivatives of the loss function, these partial derivatives produce gradients\n",
    "* use the gradients to update the values of weights and biases\n",
    "\n",
    "A more detailed yet accessible explanation may be found [here](http://ruder.io/optimizing-gradient-descent/) \n",
    "\n",
    "#### Learning rate\n",
    "The learning rate, indicates at which pace the weights get updated. It can be fixed or adaptively changed. \n",
    "\n",
    "* If the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny.\n",
    "\n",
    "* If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/0*QwE8M4MupSdqA3M4.png)\n",
    "\n",
    "[source](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)\n",
    "\n",
    "\n",
    "#### Adaptive learning rates\n",
    "Letting the learning rate vary when training a model can reduce the training time and improve the numerical optimal solution. \n",
    "\n",
    "* The Adam optimizer is the most commonly used technique. \n",
    "\n",
    "* Here we are using a technique called Root Mean Squared Propagation\" or RMSprop an adaptive version of Stochastic Gradient Descent. A more detailed yet accessible explanation may be found [here](http://ruder.io/optimizing-gradient-descent/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYL8RIydy4cF"
   },
   "source": [
    "#### Loss function\n",
    "\n",
    "Binary cross-entropy compares the log of probabilities of each predicted class with the log of the observed values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JI6KBRyphJv6"
   },
   "source": [
    "### Semantic segmentation of water using U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pae5SRNGmRtn"
   },
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhN8BkzTmT7W"
   },
   "outputs": [],
   "source": [
    "#imagery\n",
    "file_id = '1iMfIjr_ul49Ghs2ewazjCt8HMPfhY47h'\n",
    "destination = 's2cloudless_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "    \n",
    "#labels\n",
    "file_id = '1c7MpwKVejoUuW9F2UaF_vps8Vq2RZRfR'\n",
    "destination = 's2cloudless_label_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1u-hg_xmaLu"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip(f):\n",
    "    \"\"\"\n",
    "    f = file to be unzipped\n",
    "    \"\"\"    \n",
    "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        \n",
    "if colab==1:\n",
    "    unzip('s2cloudless_imagery.zip')\n",
    "    unzip('s2cloudless_label_imagery.zip')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btiDAEDfhJv6"
   },
   "source": [
    "#### Training a Unet to detect water in Sentinel2-cloudless imagery\n",
    "\n",
    "Import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_oF1cm0ZhJv7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import json, os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Anb780n7lVZw"
   },
   "source": [
    "Use the same IOU and Unet functions we used in the last Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dBcDu3mDhW6A"
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true[:,:,:,0]\n",
    "    yp0 = tf.keras.backend.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxy75EzdhZxH"
   },
   "outputs": [],
   "source": [
    "def unet(sz = (512, 512, 3)):\n",
    "  inputs = Input(sz)\n",
    "  _ = inputs\n",
    "  \n",
    "  #down sampling \n",
    "  f = 8\n",
    "  layers = []\n",
    "  \n",
    "  for i in range(0, 6):\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    layers.append(_)\n",
    "    _ = MaxPooling2D() (_)\n",
    "    f = f*2\n",
    "  ff2 = 64 \n",
    "  \n",
    "  #bottleneck \n",
    "  j = len(layers) - 1\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "  _ = Concatenate(axis=3)([_, layers[j]])\n",
    "  j = j -1 \n",
    "  \n",
    "  #upsampling \n",
    "  for i in range(0, 5):\n",
    "    ff2 = ff2//2\n",
    "    f = f // 2 \n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "    _ = Concatenate(axis=3)([_, layers[j]])\n",
    "    j = j -1 \n",
    "    \n",
    "  #classification \n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  outputs = Conv2D(1, 1, activation='sigmoid') (_)\n",
    "  \n",
    "  #model creation \n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "  \n",
    "  return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBK74wgKhjZt"
   },
   "outputs": [],
   "source": [
    "model = unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfJIzJPwhJv-"
   },
   "source": [
    "#### Checkpoints\n",
    "\n",
    "Checkpoints are a way to save the current state of your model so that you can pick up from where you left off. They capture\n",
    "\n",
    "* The architecture of the model, allowing you to re-create the model\n",
    "\n",
    "* The weights of the model (for now, these will essentially be random numbers)\n",
    "\n",
    "* The training configuration (loss, optimizer, epochs, and other meta-information - I'll explain what all this means later)\n",
    "\n",
    "With Colab, GPU kernels may terminate, suddenly and without warning. Therefore, it is often a good idea to keep number of training epochs small, but to use checkpoints\n",
    "\n",
    "Then, it looks from images and corresponding label images, and trains a deep learning model **end-to-end**. This means that all the parameters in the models will be learned from our data, using a single large network that automatically learns how to map inputs (pixels and regions of images) to outputs (our classes). This is computationally intensive, hence the need for GPUs. Our workflow would therefore be described in the DL literature as 'end-to-end'. \n",
    "\n",
    "We will are not using 'transfer learning', which is the (usually less computationally intensive) process of using not only existing models, but also re-purposing existing parameters sets learned from other data.\n",
    "\n",
    "Functions to save the model at each epoch and show some predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NxQYngti5F8"
   },
   "outputs": [],
   "source": [
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('mean_iou'))\n",
    "        self.val_acc.append(logs.get('val_mean_iou'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        infile = f's2cloudless_imagery/data/{path}'\n",
    "        raw = Image.open(infile)\n",
    "        raw = np.array(raw.resize((512, 512)))/255.\n",
    "        raw = raw[:,:,0:3]\n",
    "        \n",
    "        #predict the mask \n",
    "        pred = 255*model.predict(np.expand_dims(raw, 0)).squeeze()\n",
    "        print(np.max(pred))\n",
    "                \n",
    "        #mask post-processing \n",
    "        msk  = (pred>60).astype('int') #100       \n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "        #msk[msk >= 0.5] = 1 \n",
    "        #msk[msk < 0.5] = 0 \n",
    "        \n",
    "        #show the mask and the segmented image \n",
    "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(combined)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AjU20AKhJv-"
   },
   "outputs": [],
   "source": [
    "def build_callbacks():\n",
    "        checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='unet.h5', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "        callbacks = [checkpointer, PlotLearning()]\n",
    "        return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5UuRmK-li35"
   },
   "source": [
    "Colab users, you will need to quickly upload your copy of ```all_labels.json``` here. The json file contains the image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-f2ygB-JnL-i"
   },
   "outputs": [],
   "source": [
    "# load the labels than contain the image list\n",
    "data = json.load(open('all_labels.json'))\n",
    "images = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwJob2G4lpRW"
   },
   "source": [
    "Use the image batch generator that we used in the previous Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw2g9Kguxutz"
   },
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f's2cloudless_imagery/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('s2cloudless_imagery','s2cloudless_label_imagery')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>100).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn3ubqIQluvX"
   },
   "source": [
    "Define our one hyperparameter (batch size), the proportion of the dataset to use for training (60%), then get all the files, shuffle them randomly, draw 60% for training, then use the rest for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBt3-Ipfxuwg"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "prop_train = 0.6\n",
    "\n",
    "all_files = os.listdir('s2cloudless_imagery/data')\n",
    "shuffle(all_files)\n",
    "\n",
    "split = int(prop_train * len(all_files))\n",
    "\n",
    "#split into training and testing\n",
    "train_files = all_files[0:split]\n",
    "test_files  = all_files[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_Btx0i3TnxX"
   },
   "source": [
    "Based on these files, define train and test generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbKaXzXZxuyq"
   },
   "outputs": [],
   "source": [
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5EaM2fghJwF"
   },
   "source": [
    "We're going to define the number of steps per epoch (based on the batch size) and also the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUAwgm_-tps8"
   },
   "outputs": [],
   "source": [
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "\n",
    "print(train_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hadzGFH4j8y5"
   },
   "source": [
    "Start model training. Use `verbose = 0` because we have a callback function that displays a test image and prediction at the end of each epoch. We `use_multiprocessing = True` in case keras complains about our image generator function not being thread-safe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1X9wBF4YD0E8LKotKi-koD7jvcHfh6TVL"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2370408,
     "status": "ok",
     "timestamp": 1572389251559,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "AnQYPMg5UIBX",
    "outputId": "4b035072-5141-4393-e709-103c7458fb28"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                    epochs = 100, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bviq-2itk_DG"
   },
   "source": [
    "#### Training with image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vK5vjpAkmDwp"
   },
   "source": [
    "In the previous Part we saw how to use the ```ImageDataGenerator``` class in ```keras```, which we use below to generate batches of 1 images randomly rotated up to 45 degrees, flipped horizontally, and/or zoomed in/out up to 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "humXRTbaA47U"
   },
   "outputs": [],
   "source": [
    "# generate batches of one because the generator will be used\n",
    "# to print imagery to file\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,     \n",
    "        shear_range=0,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=45,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "img_generator = train_datagen.flow_from_directory(    \n",
    "        's2cloudless_imagery',\n",
    "        target_size=(2048, 2048), #opt for a fairly small image size for memory efficiency\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, seed=111, shuffle=False)\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,     \n",
    "        shear_range=0,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=45,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "mask_generator = test_datagen.flow_from_directory(\n",
    "        's2cloudless_label_imagery',\n",
    "        target_size=(2048, 2048),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, seed=111, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frCYrYygmloz"
   },
   "source": [
    "Convert each image and label image to floating point between 0 and 1 while combing the two generators, then generate and save to file 100 augmented images and associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbPDH4IoDwID"
   },
   "outputs": [],
   "source": [
    "n_aug_files = 100\n",
    "\n",
    "train_generator2 = (tuple(np.array(pair, dtype='float64')/255) for pair in zip(img_generator, mask_generator))\n",
    "\n",
    "counter = 0\n",
    "while counter<n_aug_files:\n",
    "    x, y = next(train_generator2)\n",
    "    matplotlib.image.imsave('s2cloudless_label_imagery'+os.sep+'data'+os.sep+\"augimage00\"+str(counter)+\".jpg_mask.jpg\", np.squeeze(y[0])) \n",
    "    matplotlib.image.imsave('s2cloudless_imagery'+os.sep+'data'+os.sep+\"augimage00\"+str(counter)+\".jpg\", np.squeeze(x[0]))    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_04-R5F3jsnT"
   },
   "source": [
    "While the above is running, you can check in the file folders to see the new augmented imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSdtoz0FVtui"
   },
   "source": [
    "(in case you want to undo the above, uncomment and run the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_WFOVC-A49l"
   },
   "outputs": [],
   "source": [
    "#! rm s2cloudless_label_imagery/data/aug*.jpg\n",
    "#! rm s2cloudless_imagery/data/aug*.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rlETbl0nZCY"
   },
   "source": [
    "Shuffle files again to pick a new set of training and testing files, and refine the test and train generators for the next round of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwD2mjs6az1n"
   },
   "outputs": [],
   "source": [
    "#batch size can now be higher because you have a function \n",
    "#generating augmented imagery\n",
    "batch_size = 16\n",
    "\n",
    "shuffle(all_files)\n",
    "\n",
    "split = int(prop_train * len(all_files))\n",
    "\n",
    "#split into training and testing\n",
    "train_files = all_files[0:split]\n",
    "test_files  = all_files[split:]\n",
    "\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "\n",
    "print('Steps per training epoch: %i' % (train_steps))\n",
    "print('Steps per testing epoch: %i' % (test_steps))\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0oTcdj2k0pk"
   },
   "source": [
    "Remove the previous model weights file and run the model training function again, this time using augmented imagery, and double the training epochs (200) to capitalize on the model having more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_KjS7TuhFcR"
   },
   "outputs": [],
   "source": [
    "os.remove('unet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9r_ydRsjsng"
   },
   "source": [
    "By using a checkpoint weights file, you could use smaller numbers of epochs and repeatedly run the cell below. However, that will meann it is harder to keep track of training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Y4vGISOV1qLe54VAOuCfqXiq8saR0_tj"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3615538,
     "status": "ok",
     "timestamp": 1572397243112,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "JuBtrS7hg-Ob",
    "outputId": "2dd223fa-5418-4092-f316-961e5dbec891"
   },
   "outputs": [],
   "source": [
    "history_aug = model.fit_generator(train_generator, \n",
    "                    epochs = 100, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1c_x8TPmhJwX"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Evaluating <br/> semantic segmentation results</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "We evaluate how good a model is at prediction using an unseen test set of images and an evaluation metric. \n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUE6pLD_hJwY"
   },
   "source": [
    "#### Examine training history\n",
    "\n",
    "`history` contains the training and validation metrics that we can then plot as a function of epoch\n",
    "\n",
    "`history_aug` is the equivalent for the second set of training using augmented data. \n",
    "\n",
    "We have both training and validation losses and mean IoU scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2uLnIjAhJwZ"
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdsgLWV06Mls"
   },
   "outputs": [],
   "source": [
    "print(history_aug.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A61towTajsnm"
   },
   "source": [
    "The plot below shows the training history of the original non-augmented and augmented training runs. You can see by comparing the four curves that augmentation doesn't helps a little with overall final training accuracy, which seems to start to plateau at around 0.85 for both augmented models. Augmentation also seemsto help the model generalize better; test set scores are generally higher at around 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmPZ7hCShJwb"
   },
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['mean_iou'],'k',lw=1) #plot the training iou curve\n",
    "plt.plot(history.history['val_mean_iou'],'r',lw=1) #plot the testing iou curve\n",
    "plt.ylim(0,1) #put limits of y axis\n",
    "plt.axhline(y=0.85) #draw a horizontal line at 85% IoU\n",
    "plt.title('Model IoU') #add title, labels and a legend\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history_aug.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(history_aug.history['val_mean_iou'],'r',lw=1)\n",
    "plt.title('Augmented model IoU')\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42zDKQUpjsnp"
   },
   "source": [
    "Augmentation improves IoU too, approaching the 5% error line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOcO3TPZhJwe"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'],'k',lw=1)\n",
    "plt.plot(history.history['val_loss'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.05)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history_aug.history['loss'],'k',lw=1)\n",
    "plt.plot(history_aug.history['val_loss'],'r',lw=1)\n",
    "plt.title('Augmented model loss')\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.05)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqqp8e4ohJwg"
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6o9k3URhJwh"
   },
   "outputs": [],
   "source": [
    "# open an image \n",
    "raw = Image.open('s2cloudless_imagery'+os.sep+'data'+os.sep+test_files[0])\n",
    "\n",
    "# resize and rescale, which is what the model is expecting as inputs\n",
    "raw = np.array(raw.resize((512, 512)))/255.\n",
    "##raw = raw[:,:,0:3]\n",
    "\n",
    "#predict the mask \n",
    "pred = model.predict(np.expand_dims(raw, 0))\n",
    "\n",
    "#mask post-processing \n",
    "msk  = pred.squeeze()\n",
    "msk = np.stack((msk,)*3, axis=-1)\n",
    "# binarize\n",
    "msk[msk >= 0.5] = 1 \n",
    "msk[msk < 0.5] = 0 \n",
    "\n",
    "#show the mask and the segmented image \n",
    "combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(combined)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Jpap945L4WO"
   },
   "source": [
    "What next?\n",
    "\n",
    "It did _reasonably_ well on the training data but not so well. We probably need more data. This is where the NWPU-RESISC45 imagery might come to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yq4Kha-0hJw4"
   },
   "source": [
    "![](https://github.com/dbuscombe-usgs/cdi_dl_workshop/raw/67e8c84d0e0b89b024814ef2e8f3bed091ee0c4e/Day2/figs/Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBaWIwVyhJw8"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">Deliverable</strong>  \n",
    "    <br/>The deliverable for Part 4 is a jupyter notebook showing a workflow to train and test U-Net models for training using augmented NWPU-RESISC45 lake images and corresponding labels, and using augmented sentinel-2 cloudless imagery and corresponding labels. So, there are two U-Net based models in total. You should compare their outputs. Finally, you should test the model trained on NWPU imagery on the s2cloudless imagery.  This will mostly test your understanding of the generic yet complex workflow of training, testing and comparing models based on a test data set, as well as how to interpret evaluation metrics. The baseline evaluation of testing the model trained on NWPU imagery on the s2cloudless imagery (in machine learning called \"transfer learning\") is what you will optimize in Part 5.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TyBR96GhJw-"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Going further: <br/> train models with your own data</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "If you developed your own imagery in Part 2, you should be able to adapt the workflow presented here to developing training and testing sets (augmented or not). You could choose to build separate UNet models for each dataset, or merge datasets together to build one 'master' UNet model. Most deep learning studies report that model performance scale with data set size.\n",
    "</p>\n",
    "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
    "Many deep learning studies report that model performance scale with model size, so you could add filters to the unet model atchitecture presented in this part\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Part4_Training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
