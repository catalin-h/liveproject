{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGhkrMQX9KPn"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification (Manning Publications)</span><br/>by <em>Daniel Buscombe</em></strong><br/><br/>\n",
    "        <strong>> Chapter 5: Model Optimization </strong><br/>\n",
    "    </p>           \n",
    "        \n",
    "<p style=\"border: 1px solid #182AEB; border-left: 15px solid #182AEB; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #182AEB\">What you learned in Part 4.</strong>  \n",
    "    <br/>In Part 4, you trained U-Net models to segment water pixels in imagery\n",
    "    </p>\n",
    "    \n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">What you will learn in this Part.</strong>  \n",
    "    <br/>In Part 5, you will optimize the models you made in Part 4. You will first learn how to tune hyper-parameters, add regularization and modify model architectures in several ways, and deal with class imbalance. Then, you will learn how to use a machine learning model known as a ‘fully-connected conditional random field” to refine label images, so labels are as accurate as possible.\n",
    "    </p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROJhVuY4_VOB"
   },
   "source": [
    "#### Preliminaries for Colab\n",
    "\n",
    "Like Part 3 and 4 previously, below are some convenience functions for those working on Google Colab with a GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48789,
     "status": "ok",
     "timestamp": 1572819087100,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "I-lc86ey-HOb",
    "outputId": "d9e25c96-3e24-41e2-95f8-ced7856f1a46"
   },
   "outputs": [],
   "source": [
    "colab = 0\n",
    "#colab = 1\n",
    "\n",
    "if colab==1:\n",
    "    %tensorflow_version 2.x\n",
    "    #!pip install --default-timeout=1000 tensorflow-gpu==2.0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8ruWT7u_ZLB"
   },
   "source": [
    "You may have to restart the runtime and/or change runtime type here, if the following doesn't show Tensorflow version 2, and a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1572819133184,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "v7gY9Wi2_aHF",
    "outputId": "3a739855-8e83-4a41-e37c-10bfac00f265"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7i8OMteVsrG"
   },
   "source": [
    "Convenience functions if you need to download example (minimal) imagery sets derived from NWPU and Sentinel-2 cloudless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt7x8mRRVs7m"
   },
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "#s2 cloudless imagery\n",
    "file_id = '1iMfIjr_ul49Ghs2ewazjCt8HMPfhY47h'\n",
    "destination = 's2cloudless_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#s2 cloudless labels\n",
    "file_id = '1c7MpwKVejoUuW9F2UaF_vps8Vq2RZRfR'\n",
    "destination = 's2cloudless_label_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu imagery\n",
    "file_id = '1gtuqy1VlU8-M5IEMnmiSuTlI5PxQPnGB'\n",
    "destination = 'nwpu_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu labels\n",
    "file_id = '1W5LGbcYAcFbG5YjLgX_ekBn0u5Rno35x'\n",
    "destination = 'nwpu_label_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sommbr0GVy_F"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip(f):\n",
    "    \"\"\"\n",
    "    f = file to be unzipped\n",
    "    \"\"\"    \n",
    "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        \n",
    "if colab==1:\n",
    "    unzip('s2cloudless_imagery.zip')\n",
    "    unzip('s2cloudless_label_imagery.zip')   \n",
    "    unzip('nwpu_images.zip')\n",
    "    unzip('nwpu_label_images.zip')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa9K6-E2T4fR"
   },
   "source": [
    "Import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cvYaMkSR-JKj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import json, os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYgpaO2O9KPr"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Training tuning strategies</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "We will set up a baseline model with no optimization using the NWPU imagery, then explore the following training optimization strategies:\n",
    "<ul>\n",
    "  <li>Building a bigger model with more layers</li>    \n",
    "  <li>Using Early Stopping and Adaptive Learning Rates</li> \n",
    "  <li>Using a bigger model (and dropout)</li> \n",
    "  <li>Using regularization (Batch Normalization)</li> \n",
    "  <li>Using residual connections</li> \n",
    "</ul>\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsLTcU6OJMAh"
   },
   "source": [
    "#### Getting things set up with a baseline model\n",
    "Zip through all these functions that we defined in the last Part. \n",
    "\n",
    "Like in the last Part, we will define the ```unet``` model, create model training callbacks, and generate augmented imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVAJUzEt-Iqq"
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true[:,:,:,0]\n",
    "    yp0 = tf.keras.backend.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou\n",
    "\n",
    "def unet(sz = (512, 512, 3)):\n",
    "  inputs = Input(sz)\n",
    "  _ = inputs\n",
    "  \n",
    "  #down sampling \n",
    "  f = 8\n",
    "  layers = []\n",
    "  \n",
    "  for i in range(0, 6):\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    layers.append(_)\n",
    "    _ = MaxPooling2D() (_)\n",
    "    f = f*2\n",
    "  ff2 = 64 \n",
    "  \n",
    "  #bottleneck \n",
    "  j = len(layers) - 1\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "  _ = Concatenate(axis=3)([_, layers[j]])\n",
    "  j = j -1 \n",
    "  \n",
    "  #upsampling \n",
    "  for i in range(0, 5):\n",
    "    ff2 = ff2//2\n",
    "    f = f // 2 \n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "    _ = Concatenate(axis=3)([_, layers[j]])\n",
    "    j = j -1 \n",
    "    \n",
    "  #classification \n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  outputs = Conv2D(1, 1, activation='sigmoid') (_)\n",
    "  \n",
    "  #model creation \n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "  \n",
    "  return model \n",
    "\n",
    "model = unet()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNDTrZom9KPt"
   },
   "outputs": [],
   "source": [
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('mean_iou'))\n",
    "        self.val_acc.append(logs.get('val_mean_iou'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        infile = f'nwpu_images/data/{path}'\n",
    "        raw = Image.open(infile)\n",
    "        raw = np.array(raw.resize((512, 512)))/255.\n",
    "        raw = raw[:,:,0:3]\n",
    "        \n",
    "        #predict the mask \n",
    "        pred = 255*model.predict(np.expand_dims(raw, 0)).squeeze()\n",
    "        print(np.max(pred))\n",
    "                \n",
    "        #mask post-processing \n",
    "        msk  = (pred>60).astype('int') #100       \n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "        #msk[msk >= 0.5] = 1 \n",
    "        #msk[msk < 0.5] = 0 \n",
    "        \n",
    "        #show the mask and the segmented image \n",
    "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(combined)\n",
    "        plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkJBCYCWM6qi"
   },
   "source": [
    "We use Keras callbacks to implement learning rate decay if the validation loss does not improve for 5 continues epochs. Called \"reduce loss on plateau\"\n",
    "\n",
    "Also, we implement early stopping if the validation loss does not improve for 5 continuous epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fo2ls1vA9KPy"
   },
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f'nwpu_images/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('nwpu_images','nwpu_label_images')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>200).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WR_LULokT4ff"
   },
   "source": [
    "We've seen code like the below in the previous Part, setting up batch size, proportion of the dataset to train with, getting randomized lists of test and train file names, and finally setting up generators for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3Fu3vdZ-nsN"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "prop_train = 0.6\n",
    "\n",
    "all_files = os.listdir('nwpu_images/data')\n",
    "shuffle(all_files)\n",
    "\n",
    "split = int(prop_train * len(all_files))\n",
    "\n",
    "#split into training and testing\n",
    "train_files = all_files[0:split]\n",
    "test_files  = all_files[split:]\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2U2y96CiT4fi"
   },
   "source": [
    "A customary check that things worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1572819501376,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "8rIwVxdIIWEV",
    "outputId": "3e65d04f-eeef-4bf4-e0b9-690f7e88ff60"
   },
   "outputs": [],
   "source": [
    "x, y = next(train_generator)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), cmap='gray', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCtvJfY6T4fn"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: Using Early Stopping<br/>and Adaptive Learning Rates</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    Neural networks can overfit if they train for too long, but it's hard to know how many epochs is too many without a lot of trial and error, which is not efficient. Automatically computing how many training epochs a model needs or, to put it another way, evaluating when to stop training or <em>early stopping</em>, is achieved by monitoring validation loss during training. If loss doesn't improve for a certain number of epochs, called the <em>patience</em>, model training is terminated. Below <em>patience</em> is set to 5. Another common training strategy to prevent overly long training times is to adaptively change the learning rate during model training. This is achieved by starting with a relatively large number and then decreasing it after every training epoch until a specified minimum <em>min_lr</em> at a certain rate of reduction <em>min_delta</em>.</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ajq5OJsUT4fo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01eq594-NkqB"
   },
   "outputs": [],
   "source": [
    "def build_callbacks(filepath, min_delta, min_lr, factor):\n",
    "\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", \n",
    "                                  mode=\"min\", patience=5) \n",
    "    \n",
    "    # reduction of learning rate if and when validation scores plateau upon successive epochs\n",
    "    reduceloss_plat = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=5, \n",
    "                                    verbose=1, mode='auto', min_delta=min_delta, \n",
    "                                    cooldown=5, min_lr=min_lr)\n",
    "\n",
    "    # set checkpoint file \n",
    "    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                   verbose=0, save_best_only=True, mode='min', \n",
    "                                   save_weights_only = True)\n",
    "        \n",
    "    callbacks = [model_checkpoint, reduceloss_plat, earlystop, PlotLearning()]\n",
    "\n",
    "    return callbacks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKPDSbt3T4ft"
   },
   "outputs": [],
   "source": [
    "# a tolerance for the training.\n",
    "min_delta = 0.0001\n",
    "\n",
    "# minimum learning rate (lambda)\n",
    "min_lr = 0.0001\n",
    "\n",
    "# the factor applied to the learning rate when the appropriate triggers are made\n",
    "factor = 0.8\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "filepath = 'unet'+str(batch_size)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1572819509390,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "haW5J3F1-xax",
    "outputId": "5dd61647-fcd7-4fb9-f83c-84cc6a78834f"
   },
   "outputs": [],
   "source": [
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1h1kkvccOC8dCd-3vo-Bq0MZ5Jxx-pYuS"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69493,
     "status": "ok",
     "timestamp": 1572821080699,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "Rxoznmh4T4fy",
    "outputId": "7e25f251-4075-4f2b-c435-1711b18c2b98"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                    epochs = 50, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6Q5sqnET4f1"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 438,
     "status": "error",
     "timestamp": 1572821081133,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "vN6csuPg-xdg",
    "outputId": "8b2f9296-83bb-421d-d39e-56846a135317"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: changing model architecture <br/>use a bigger rocket</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    We'll create a new `unet` function that includes initial filter size <em>f</em> and \"bottleneck-creation filter size\" <em>ff2</em>. The the previous models, <em>f</em>=8 and <em>ff2</em>=64. This time we make that a user-decision. We'll use <em>f</em>=16 and <em>ff2</em>=128. Because this will massively increase the number of tunable parameters, we will implement a <em>regularization</em> strategy called <em>Dropout</em>. Regularization refers to a training strategy that prevents the model from overfitting the data. This ensures the model generalizes well and is useful for prediction on unseen test data. Dropout is a popular regularization trick, which involves randomly dropping neurons of output layers. The hyperparameter that governs its behavior is a rate. 0.25 means that 25% of neurons, randomly selected, will be dropped. Dropout forces the model to learn robust features, because dropout may deactivate a neuron heavily weighted for a specific feature seen only in training imagery. Dropout thus ensures that more neurons share feature extraction tasks.</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCjxXKPOT4f6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1572821751388,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "wirZ_FaMT4f9",
    "outputId": "48518d4a-58a7-4e16-c9ed-249019a70920"
   },
   "outputs": [],
   "source": [
    "def unet(sz, f, ff2):\n",
    "  inputs = Input(sz)\n",
    "  _ = inputs\n",
    "  \n",
    "  #down sampling \n",
    "  #f = 8\n",
    "  layers = []\n",
    "  \n",
    "  for i in range(0, 6):\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    layers.append(_)\n",
    "    _ = MaxPooling2D() (_)\n",
    "    _ = Dropout(0.25)(_) #added Dropout layer\n",
    "    f = f*2\n",
    "  #ff2 = 64 \n",
    "  \n",
    "  #bottleneck \n",
    "  j = len(layers) - 1\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "  _ = Concatenate(axis=3)([_, layers[j]])\n",
    "  j = j -1 \n",
    "      \n",
    "  #upsampling \n",
    "  for i in range(0, 5):\n",
    "    ff2 = ff2//2\n",
    "    f = f // 2 \n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "    _ = Concatenate(axis=3)([_, layers[j]])\n",
    "    _ = Dropout(0.25)(_) #added Dropout layer   \n",
    "    j = j -1 \n",
    "    \n",
    "  #classification \n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  outputs = Conv2D(1, 1, activation='sigmoid') (_)\n",
    "  \n",
    "  #model creation \n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "  \n",
    "  return model \n",
    "\n",
    "model = unet((512, 512, 3), 16, 128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrtHXjL7T4gE"
   },
   "source": [
    "From 6 to 26 million parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1572821752259,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "Jrw1HaHmT4gE",
    "outputId": "adc4855e-4104-4581-b2d3-c98b8c4db14b"
   },
   "outputs": [],
   "source": [
    "# since the model is much larger the batch size should be smaller so training fits in GPU memory\n",
    "batch_size = 2\n",
    "# define file path\n",
    "filepath = 'large_unet'+str(batch_size)+'.h5'\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "17cybBnRF-MdihkUJOxgQmy_l1Kq5S9L0"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4098707,
     "status": "ok",
     "timestamp": 1572825851539,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "XKeS16YPT4gH",
    "outputId": "5f124c0f-57c9-4a96-ae66-1cb76ca2a614"
   },
   "outputs": [],
   "source": [
    "large_history = model.fit_generator(train_generator, \n",
    "                    epochs = 50, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBFLj9hcT4gJ"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1572825984004,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "lKnztOBpT4gL",
    "outputId": "0a474600-db20-4d72-9724-fc088a1c1dc1"
   },
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Baseline IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(large_history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(large_history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Large model IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpV5gXxVT4gO"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: changing model architecture <br/>using Batch Normalization</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "This operation takes the outputs of the previous layer and substracts the mean and divides by the batch standard deviation. In other words, it normalizes each sample according to the standard deviation of the batch. Because batches are randomly drawn, the result is that each individual image is normalized slightly differently each time it is included in a batch. Network training benefits because this is another regularization strategy; the model learsn how to deal with intensity fluctuations in input image batches</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g8-cmL_T4gO"
   },
   "source": [
    "Update the model with batch normalization, so start by importing the relevant layer from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVVvrgNaT4gP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHd45fMiT4gR"
   },
   "source": [
    "The dropout layer (with a rate of 0.25) is added after each max pooling layer in the encoder portion of the network, and  then after the concatenation layer in the decoder portion of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbanZfU6T4gS"
   },
   "outputs": [],
   "source": [
    "def unet(sz, f, ff2):\n",
    "  inputs = Input(sz)\n",
    "  _ = inputs\n",
    "  \n",
    "  #down sampling \n",
    "  #f = 8\n",
    "  layers = []\n",
    "  \n",
    "  for i in range(0, 6):\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = BatchNormalization()(_) # added BN layer\n",
    "    layers.append(_)\n",
    "    _ = MaxPooling2D() (_)\n",
    "    _ = Dropout(0.25)(_) #added Dropout layer\n",
    "    f = f*2\n",
    "  #ff2 = 64 \n",
    "  \n",
    "  #bottleneck \n",
    "  j = len(layers) - 1\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "  _ = Concatenate(axis=3)([_, layers[j]])\n",
    "  j = j -1 \n",
    "      \n",
    "  #upsampling \n",
    "  for i in range(0, 5):\n",
    "    ff2 = ff2//2\n",
    "    f = f // 2 \n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "    _ = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_)\n",
    "    _ = Concatenate(axis=3)([_, layers[j]])\n",
    "    _ = Dropout(0.25)(_) #added Dropout layer   \n",
    "    j = j -1 \n",
    "    \n",
    "  #classification \n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  _ = Conv2D(f, 3, activation='relu', padding='same') (_)\n",
    "  outputs = Conv2D(1, 1, activation='sigmoid') (_)\n",
    "  \n",
    "  #model creation \n",
    "  model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "  \n",
    "  return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1543,
     "status": "ok",
     "timestamp": 1572825995128,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "2YBxGD4TT4gU",
    "outputId": "1b9b8179-7af6-4433-b663-957b485fafbb"
   },
   "outputs": [],
   "source": [
    "model = unet((512, 512, 3), 8, 64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T9vW4jzFT4gX",
    "outputId": "6c4ac5bb-4c32-4b84-f273-959fe4c8b801"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "filepath = 'bn_unet'+str(batch_size)+'.h5'\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)\n",
    "\n",
    "bn_history = model.fit_generator(train_generator, \n",
    "                    epochs = 50, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470,
     "status": "error",
     "timestamp": 1572821732321,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "b2CvSF0yT4ga",
    "outputId": "adca86d7-8415-40eb-cee6-d4ddab6d0af9"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awiZI4ufT4ge"
   },
   "source": [
    "Compare the training histories of the baseline and batch normalization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1192,
     "status": "error",
     "timestamp": 1572821733661,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "KVcAo-TKT4gf",
    "outputId": "936839c7-1c43-4670-ee93-a60ec418c4b4"
   },
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Baseline IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(bn_history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(bn_history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Baseline model with batch normalizaton IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmoMd5t7T4gi"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: changing model architecture <br/>using residual connections</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "A standard approach is to pass the input image goes through multiple convolutions and obtain high-level features. In a network architecture with <em>residual layers</em> or connections, each layer gets to see both the output from the previous layer (standard) as well as the inputs to that layer. So it not only sees the ouputs but also the data used to learn that output\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "566oTTdYT4gj"
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1000/1*4wx7szWCBse9-7eemGQJSw.png)\n",
    "\n",
    "Import `Activation` and `Add` layers from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUPYE3s5T4gj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7B6b4HWT4gm"
   },
   "source": [
    "Create a new UNet model function. This time we'll use a few convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_act(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation(\"relu\")(x)\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = batchnorm_act(x)\n",
    "    return Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "\n",
    "def bottleneck_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([conv, bottleneck])\n",
    "\n",
    "def res_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([bottleneck, res])\n",
    "\n",
    "def upsamp_concat_block(x, xskip):\n",
    "    u = UpSampling2D((2, 2))(x)\n",
    "    return Concatenate()([u, xskip])\n",
    "\n",
    "def res_unet(sz, f):\n",
    "    inputs = Input(sz)\n",
    "    \n",
    "    ## downsample  \n",
    "    e1 = bottleneck_block(inputs, f); f = int(f*2)\n",
    "    e2 = res_block(e1, f, strides=2); f = int(f*2)\n",
    "    e3 = res_block(e2, f, strides=2); f = int(f*2)\n",
    "    e4 = res_block(e3, f, strides=2); f = int(f*2)\n",
    "    _ = res_block(e4, f, strides=2)\n",
    "    \n",
    "    ## bottleneck\n",
    "    b0 = conv_block(_, f, strides=1)\n",
    "    _ = conv_block(b0, f, strides=1)\n",
    "    \n",
    "    ## upsample\n",
    "    _ = upsamp_concat_block(_, e4)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e3)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e2)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e1)\n",
    "    _ = res_block(_, f)\n",
    "    \n",
    "    ## classify\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(_)\n",
    "    \n",
    "    #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89fLigFvT4gp",
    "outputId": "f7d440da-df06-4a44-c784-fd53f38debc2"
   },
   "outputs": [],
   "source": [
    "model = res_unet((512, 512, 3), 16)\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gi-3jDDT4gs"
   },
   "source": [
    "Train the model with the residual connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlzUHZh6T4gt",
    "outputId": "32c1bc97-774b-41c7-f630-0dfb8f63a1ed"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "filepath = 'res_unet'+str(batch_size)+'.h5'\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)\n",
    "\n",
    "res_history = model.fit_generator(train_generator, \n",
    "                    epochs = 100, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzLf-obYT4gx"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0iAQfM8T4g0"
   },
   "source": [
    "Compare the training histories of the baseline and U-Net with residual layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CriGrS_ZT4g1",
    "outputId": "273c1990-cdcf-4475-9a21-7e25ca082fc2"
   },
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Baseline IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(res_history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Res-UNet IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdjw9DNG9KRE"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: Dealing with class imbalance <br/> using dice loss</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    the network tends to <b>ignore smaller classes</b>. A soft dice loss could be used to train a model. Unlike the IoU metric, the numerator is the number of correctly classified pixels, and the denominator is the total number of pixels in a class that is in both estimated and ground truth. Thus it is insensitive to the number of pixels total in each class.  \n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb_wsYWCT4g3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyYChIOdT4g5"
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = tf.reshape(tf.dtypes.cast(y_true, tf.float32), [-1])\n",
    "    y_pred_f = tf.reshape(tf.dtypes.cast(y_pred, tf.float32), [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olAadtwFT4g7",
    "outputId": "a655566d-723d-47e6-811d-f3a2b0e9b590"
   },
   "outputs": [],
   "source": [
    "model = res_unet((512, 512, 3), 32)\n",
    "model.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDqBeG4ZT4g-"
   },
   "source": [
    "Before we start training we need to update out `PlotLearning` class with the new loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ji0zyJa1T4g-"
   },
   "outputs": [],
   "source": [
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('dice_coef'))\n",
    "        self.val_acc.append(logs.get('val_dice_coef'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'dice_coef=',logs.get('dice_coef'),'val_dice_coef=',logs.get('val_dice_coef'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        infile = f'nwpu_images/data/{path}'\n",
    "        raw = Image.open(infile)\n",
    "        raw = np.array(raw.resize((512, 512)))/255.\n",
    "        raw = raw[:,:,0:3]\n",
    "        \n",
    "        #predict the mask \n",
    "        pred = 255*model.predict(np.expand_dims(raw, 0)).squeeze()\n",
    "        print(np.max(pred))\n",
    "                \n",
    "        #mask post-processing \n",
    "        msk  = (pred>60).astype('int') #100       \n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "        \n",
    "        #show the mask and the segmented image \n",
    "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(combined)\n",
    "        plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQtTKDCPT4hG"
   },
   "source": [
    "Train the Residual-UNet model again, this time with dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2NLNAsTT4hH",
    "outputId": "ff931dfa-76e3-44f0-ad8c-e4cf641f44cf"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "filepath = 'res_dice_unet'+str(batch_size)+'.h5'\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)\n",
    "\n",
    "res_dice_history = model.fit_generator(train_generator, \n",
    "                    epochs = 100, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLA1DE1sT4hK",
    "outputId": "83d50178-30d8-4117-cac7-e8b85b073418"
   },
   "outputs": [],
   "source": [
    "res_dice_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "toIXIsCIT4hM",
    "outputId": "7ffa0b4c-2102-46d3-941d-32fee41bd79a"
   },
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(res_history.history['mean_iou'],'k',lw=1)\n",
    "plt.plot(res_history.history['val_mean_iou'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Res-UNet IoU')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_dice_history.history['dice_coef'],'k',lw=1)\n",
    "plt.plot(res_dice_history.history['val_dice_coef'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.title('Res-UNet dice coefficient')\n",
    "plt.ylabel('IoU')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYBMlohv9KP2"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: Refining label images <br/> using conditional random fields</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "It is common to use some form of post-processing algorithm to refine either labels for training, or model predictions, on an image-by-image basis. A popular approach is called a fully connected or dense conditional random field or CRF. Each image is paired with a label image and the label image is refined based on the image. The CRF makes a model for the label given the image, then assesses the likelihood of each label given the CRF model. The model can be based on relative color differences between a label and the CRF model's idea of what color that label is (within a tolerance), and/or relative spatial differences (i.e. location within the image) between a given label and the CRF model's idea of the location of that label, again within a tolerance.\n",
    "</p>\n",
    "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
    "    We're using the CRF to correct a pixelwise (dense) label image. It can also be used as an 'inpainting' algorithm if only sparse areas of the image have labels. This was the approach proposed by <a href=\"https://www.mdpi.com/2076-3263/8/7/244\">Buscombe and Ritchie (2018)</a> \n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJQcdKlET4hQ"
   },
   "source": [
    "Uncomment below to install the module and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeQEUblY9KP3"
   },
   "outputs": [],
   "source": [
    "#!pip install cython\n",
    "#!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ebw5D7L9KP6"
   },
   "outputs": [],
   "source": [
    "from pydensecrf import densecrf\n",
    "from pydensecrf.utils import unary_from_labels\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will refine a label image based on an undelying image, using a dense CRF. The model's hyperparameters (i.e. specified by you, not by training) are:\n",
    " \n",
    "```compat_spat``` is a non-dimensional parameter that penalizes small pieces of segmentation that are spatially isolated -- it enforces more spatially consistent segmentations. Larger values means larger pieces of segmentation are allowed.\n",
    "\n",
    "```compat_col``` is a non-dimensional parameter that penalizes pieces of segmentation that are less uniform in color -- it enforces more consistent segmentations in colorspace. Larger values means pieces of segmentation with less similar image intesity are allowed.\n",
    "\n",
    "`theta_spat` and `theta_col` are tolerances in location and intensity, respectively. Larger values means pixel pairs can be considered to be the same class label with less similar location or intensity.\n",
    "\n",
    "`num_iter` is the number of iterations to run the CRF model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxLTPfE-9KP-"
   },
   "outputs": [],
   "source": [
    "def crf_labelrefine(input_image, predicted_labels):\n",
    "    \n",
    "    compat_spat=10\n",
    "    compat_col=30\n",
    "    theta_spat = 20\n",
    "    theta_col = 80\n",
    "    num_iter = 7\n",
    "    \n",
    "    h, w = input_image.shape[:2] #get image dimensions\n",
    "    \n",
    "    d = densecrf.DenseCRF2D(w, h, 2) #create a CRF object\n",
    "\n",
    "    # For the predictions, densecrf needs 'unary potentials' which are labels (water or no water)\n",
    "    predicted_unary = unary_from_labels(predicted_labels.astype('int')+1, num_classes, gt_prob= 0.51)\n",
    "    \n",
    "    # set the unary potentials to CRF object\n",
    "    d.setUnaryEnergy(predicted_unary)\n",
    "\n",
    "    # to add the color-independent term, where features are the locations only:\n",
    "    d.addPairwiseGaussian(sxy=(theta_spat, theta_spat), compat=compat_spat, kernel=densecrf.DIAG_KERNEL,\n",
    "                          normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    input_image_uint = (input_image * 255).astype(np.uint8) #enfore unsigned 8-bit\n",
    "    # to add the color-dependent term, i.e. 5-dimensional features are (x,y,r,g,b) based on the input image:    \n",
    "    d.addPairwiseBilateral(sxy=(theta_col, theta_col), srgb=(5, 5, 5), rgbim=input_image_uint,\n",
    "                           compat=compat_col, kernel=densecrf.DIAG_KERNEL, \n",
    "                           normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # Finally, we run inference to obtain the refined predictions:\n",
    "    refined_predictions = np.array(d.inference(num_iter)).reshape(num_classes, h, w)\n",
    "    \n",
    "    # since refined_predictions will be a 2 x width x height array, \n",
    "    # each slice respresenting probability of each class (water and no water)\n",
    "    # therefore we return the argmax over the zeroth dimension to return a mask\n",
    "    return np.argmax(refined_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and, like before, compile it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_unet((512, 512, 3), 32)\n",
    "model.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch of images and associated label images, use the model to predict the label for the first image. Then refine that label using the CRF post-processing function. Run the cell repeatedly to see a variety of outputs and CRF refinements. How does the CRF model do? Try varying the model hyperparameters to see if you can make improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train_generator)\n",
    "ypred = model.predict(tf.expand_dims(x[0], axis=0))\n",
    "ypred_crf = crf_labelrefine(x[0], ypred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use contours to visually compare the lake shorelines from ground truth (blue), model prediction (red) and CRF post-processing of the model prediction (white), then delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(x[0])\n",
    "plt.contour(y[0].squeeze(), alpha=0.5, colors='b', label='Ground truth')\n",
    "plt.contour(ypred.squeeze(), alpha=0.5, colors='r', label='Prediction')\n",
    "plt.contour(ypred_crf.squeeze(), alpha=0.5, colors='w', label = 'CRF prediction')\n",
    "plt.axis('off')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen this function before, but this time we'll need to add a line that carries out the CRF post-processing on the input (training) mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f'nwpu_images/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('nwpu_images','nwpu_label_images')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>200).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "        \n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        mask = crf_labelrefine(raw, mask).squeeze()\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also seen this function many times before, but again we'll need to add a line that carries out the CRF post-processing on the output mask (estimated by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('dice_coef'))\n",
    "        self.val_acc.append(logs.get('val_dice_coef'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'dice_coef=',logs.get('dice_coef'),'val_dice_coef=',logs.get('val_dice_coef'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        infile = f'nwpu_images/data/{path}'\n",
    "        raw = Image.open(infile)\n",
    "        raw = np.array(raw.resize((512, 512)))/255.\n",
    "        raw = raw[:,:,0:3]\n",
    "        \n",
    "        #predict the mask \n",
    "        pred = 255*model.predict(np.expand_dims(raw, 0)).squeeze()\n",
    "        print(np.max(pred))\n",
    "                \n",
    "        #mask post-processing \n",
    "        msk  = (pred>60).astype('int') #100    \n",
    "        \n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        msk = crf_labelrefine(raw, msk).squeeze()\n",
    "      \n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "          \n",
    "        #show the mask and the segmented image \n",
    "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(combined)\n",
    "        plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model like previously. This time both input and output masks of an image will be hopefully improved using the CRF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "filepath = 'res_dice_crf_unet'+str(batch_size)+'.h5'\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)\n",
    "\n",
    "res_dice_history = model.fit_generator(train_generator, \n",
    "                    epochs = 100, steps_per_epoch = train_steps,\n",
    "                    validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(filepath, min_delta, min_lr, factor), verbose = 0,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and validation loss, and dice coefficients to check the model training outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for iou\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(res_history.history['loss'],'k',lw=1)\n",
    "plt.plot(res_history.history['val_loss'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_dice_history.history['dice_coef'],'k',lw=1)\n",
    "plt.plot(res_dice_history.history['val_dice_coef'],'r',lw=1)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=0.85)\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: <br/> using ensemble predictions</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    It is common to train several models and average their predictions. This is called <b>ensemble</b> modeling because you are using the group of models to make a single prediction. \n",
    "</p>\n",
    "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
    "    We have several trained models. Here we will apply them all to the same imagery and average the masks, to see if that produces a more stable estimate\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two models, compile them, and assign them the weights from two different 'h5' files saved during model training using the `load_weights` utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = res_unet((512, 512, 3), 32)\n",
    "model1.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model1.load_weights('weights/res_dice_crf_unet2.h5')\n",
    "\n",
    "model2 = res_unet((512, 512, 3), 32)\n",
    "model2.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model2.load_weights('weights/res_dice_unet2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for 1) getting estimates from models 1 and 2; 2) take the pixelwise maximum of the two; 3) obtain a CRF-refined estimated water mask, and 4) compute an IOU score evaluated against the real mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(x, y, model1, model2):\n",
    "    #predict the mask \n",
    "    pred1 = model1.predict(np.expand_dims(x, 0))\n",
    "    pred2 = model2.predict(np.expand_dims(x, 0))\n",
    "    \n",
    "    #mask post-processing \n",
    "    msk  = np.maximum(pred1.squeeze(), pred2.squeeze())\n",
    "    # binarize\n",
    "    msk[msk >= 0.5] = 1 \n",
    "    msk[msk < 0.5] = 0\n",
    "    \n",
    "    # use CRF to refine mask before it is used as a label\n",
    "    msk = crf_labelrefine(x.squeeze(), msk).squeeze()\n",
    "        \n",
    "    # return the prediction and the IOU score of the prediction\n",
    "    return msk, mean_iou(y, msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch of images and labels and run the `get_pred` command on the first pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(image_batch_generator)\n",
    "ypred = get_pred(x[0], y[0], model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the label mask and the ensemble estimate side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), cmap='gray', alpha=0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(ypred.squeeze(), cmap='gray', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7pff5C89KRM"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">Deliverable</strong>  \n",
    "    <br/>The deliverable for Part 5 is a jupyter notebook showing a workflow to optimize the training of a model using the NWPU-RESISC45 lake images and corresponding labels, for the purposes of estimating lake area over time at sites represented in the Sentinel-2 imagery. The notebook will also show how to refine image labels based on a CRF model using both image color and relative spatial location in the image to make predictions, using optimized tunable parameters. This will mostly test your understanding of the generic yet complex workflow of optimizing model training, and applying a model trained on one dataset to another similar dataset, for the operational purpose of developing a time-series of lake areas at critical sites. \n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part5_Optimizing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
